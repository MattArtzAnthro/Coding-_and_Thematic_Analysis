{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coding and Thematic Analysis\n",
        "\n",
        "Created by [Matt Artz](https://www.mattartz.me/) ‚Äî Advancing AI Anthropology through computational approaches to qualitative research.\n",
        "\n",
        "## What This Tool Does\n",
        "\n",
        "This notebook analyzes qualitative data by coding it using both deductive and inductive approaches, and then building those codes up into themes. Rather than manually applying codes to hundreds of text segments and then building themes iteratively, you receive coded data as well as suggested themes.\n",
        "\n",
        "Building on the foundations established by the Qualitative Codebook Builder and Interview Transcript Semantic Chunker, the notebook processes chunked text by applying the pre-defined deductive codes while inductively identifying emergent codes, and then it constructs themes from both approaches.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Data Integration**: Imports codebooks and transcripts from other AI Anthropology Toolkit notebooks\n",
        "- **Three Coding Approaches**: Choose between deductive, inductive, or hybrid coding methodologies\n",
        "- **Code Application**: Uses Claude AI for coding across text segments\n",
        "- **Theme Construction**: Builds hierarchical themes from coded data\n",
        "- **Export Options**: Produces Excel workbooks and formatted Word documents with themes\n",
        "\n",
        "\n",
        "## Workflow\n",
        "\n",
        "1. Import deductive codebook from Qualitative Codebook Builder (CSV format)\n",
        "2. Import interview chunks from Interview Transcript Processor (CSV format)\n",
        "3. Configure analysis parameters including coding approach and AI model\n",
        "4. Apply deductive codes using predefined codes (if selected)\n",
        "5. Discover inductive codes emerging from the data (if selected)\n",
        "6. Integrate codes and build themes\n",
        "7. Generate visualizations and export results\n",
        "\n",
        "## Applications\n",
        "\n",
        "This tool supports qualitative research from dissertation fieldwork to applied research projects. It's particularly useful for computational analysis using the tools in my AI Anthropology Toolkit, enabling researchers to maintain rigor while working at scale.\n",
        "\n",
        "## Methodological Positioning\n",
        "\n",
        "This represents \"computational anthropology\" - using AI to enhance rather than replace traditional qualitative methods. The tool handles the time-intensive coding and thematic analysis process of interpretation and meaning-making.\n",
        "\n",
        "**Important:** AI can assist with pattern recognition and consistency, but human expertise remains essential for contextual understanding.\n",
        "\n",
        "## Target Audience\n",
        "\n",
        "Designed for anthropologists and qualitative researchers working with qualitative data‚Äîfrom graduate students managing thesis interviews to research teams processing large datasets for applied projects.\n",
        "\n",
        "## Technical Approach\n",
        "\n",
        "Combines natural language processing with anthropological coding principles, using Claude AI to apply codes consistently while discovering emergent patterns through computational analysis.\n",
        "\n",
        "## Contributing to AI Anthropology\n",
        "\n",
        "This notebook contributes to the emerging field of AI Anthropology‚Äîwhich combines studying AI as cultural artifact, using AI to enhance ethnographic research, and applying anthropological insights to AI development (Artz, forthcoming). By open-sourcing these tools, this work advances the collective capacity of anthropologists to work effectively with computational methods.\n",
        "\n",
        "## AI Anthropology Toolkit\n",
        "\n",
        "This tool is part of a growing suite of computational resources for anthropological research:\n",
        "\n",
        "\n",
        "- **[Qualitative Codebook Builder](https://github.com/MattArtzAnthro/Qualitative_Codebook_Builder)** - AI-assisted development of qualitative coding frameworks\n",
        "- **[Interview Transcript Semantic Chunker](https://github.com/MattArtzAnthro/Interview_Transcript_Semantic_Chunker)** - AI-assisted segmentation of interview transcripts\n",
        "- **[Coding and Thematic Analysis](https://github.com/MattArtzAnthro/Coding_and_Thematic_Analysis)** (this tool) - AI-assisted coding and thematic analysis of qualtiative data\n",
        "\n",
        "*Additional tools will be added to this toolkit as they are developed.*\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. You may remix, adapt, and build upon the material for non-commercial purposes, provided you credit Matt Artz and link to the repository.\n",
        "\n",
        "**Full license details**: https://creativecommons.org/licenses/by-nc/4.0/\n",
        "\n",
        "## Attribution   \n",
        "\n",
        "If you use or adapt this project in your work, please cite:\n",
        "\n",
        "\n",
        "> Built with the Coding and Thematic Analysis (Matt Artz, 2025) ‚Äî https://github.com/MattArtzAnthro/Coding_and_Thematic_Analysis\n",
        "\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this tool in your academic research, please cite:\n",
        "\n",
        "\n",
        "> Artz, Matt. 2025. Coding and Thematic Analysis. Software.\n",
        "Zenodo. https://doi.org/10.5281/zenodo.15832611\n",
        "\n",
        "## Refrences\n",
        "Artz, Matt. Forthcoming. ‚ÄúAI Anthropology: The Future of Applied Anthropological Practice.‚Äù In Routledge Handbook of Applied Anthropology, edited by Christina Wasson, Edward B. Liebow, Karine L. Narahara, Ndukuyakhe Ndlovu, and Alaka Wali. New York: Routledge."
      ],
      "metadata": {
        "id": "_VRoP3r_b9tV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Installation\n",
        "\n",
        "Install required Python packages and import necessary libraries for mixed-method qualitative analysis. This includes AI integration, data processing, visualization, and export capabilities."
      ],
      "metadata": {
        "id": "2PehyIVMcREm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BycpymR4H532"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install anthropic pandas numpy matplotlib seaborn wordcloud plotly networkx openpyxl python-docx ipywidgets scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "from collections import Counter, defaultdict\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import networkx as nx\n",
        "\n",
        "# Document generation\n",
        "from docx import Document\n",
        "from docx.shared import Inches, Pt, RGBColor\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "from docx.enum.style import WD_STYLE_TYPE\n",
        "\n",
        "# UI components\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML, clear_output\n",
        "\n",
        "# Anthropic for Claude API\n",
        "import anthropic\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "print(\"üìä AI-Assisted Qualitative Coding Analyzer\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Setup completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(\"\\n‚úÖ All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuration Interface\n",
        "\n",
        "Configure all analysis parameters including file uploads, coding approach, AI model selection, and API settings through an intuitive interface."
      ],
      "metadata": {
        "id": "SHHMKa64EFVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global configuration storage\n",
        "config = {\n",
        "    'codebook_df': None,\n",
        "    'transcript_df': None,\n",
        "    'coding_approach': 'hybrid',\n",
        "    'ai_model': 'claude-4-sonnet-20250514',\n",
        "    'api_key': '',\n",
        "    'output_folder': 'coding_analysis_output',\n",
        "    'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "}\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(config['output_folder'], exist_ok=True)\n",
        "\n",
        "# File upload widgets\n",
        "codebook_upload = widgets.FileUpload(\n",
        "    accept='.csv',\n",
        "    multiple=False,\n",
        "    description='Codebook CSV:',\n",
        "    button_style='primary',\n",
        "    style={'button_color': '#6096BA'}\n",
        ")\n",
        "\n",
        "transcript_upload = widgets.FileUpload(\n",
        "    accept='.csv',\n",
        "    multiple=False,\n",
        "    description='Transcript CSV:',\n",
        "    button_style='primary',\n",
        "    style={'button_color': '#6096BA'}\n",
        ")\n",
        "\n",
        "# Coding approach selection\n",
        "coding_approach = widgets.RadioButtons(\n",
        "    options=['Deductive Only', 'Inductive Only', 'Hybrid (Both)'],\n",
        "    value='Hybrid (Both)',\n",
        "    description='',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# AI model selection\n",
        "ai_model_dropdown = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Claude 4 Sonnet (Latest)', 'claude-4-sonnet-20250514'),\n",
        "        ('Claude 3.5 Sonnet', 'claude-3-5-sonnet-20241022')\n",
        "    ],\n",
        "    value='claude-4-sonnet-20250514',\n",
        "    description='Claude Model:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# API key input\n",
        "api_key_input = widgets.Password(\n",
        "    placeholder='Enter your Anthropic API key',\n",
        "    description='API Key:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# Status output\n",
        "status_output = widgets.Output()\n",
        "\n",
        "# Process buttons\n",
        "test_button = widgets.Button(\n",
        "    description='üß™ Test Setup',\n",
        "    button_style='info',\n",
        "    style={'button_color': '#A3CEF1'}\n",
        ")\n",
        "\n",
        "process_button = widgets.Button(\n",
        "    description='‚úÖ Apply Configuration',\n",
        "    button_style='success',\n",
        "    style={'button_color': '#6096BA'}\n",
        ")\n",
        "\n",
        "def test_setup(b):\n",
        "    with status_output:\n",
        "        clear_output(wait=True)\n",
        "        print(\"üß™ Testing configuration...\")\n",
        "\n",
        "        # Test file uploads\n",
        "        codebook_ok = bool(codebook_upload.value)\n",
        "        transcript_ok = bool(transcript_upload.value)\n",
        "        api_ok = bool(api_key_input.value)\n",
        "\n",
        "        print(f\"üìä Codebook file: {'‚úÖ' if codebook_ok else '‚ùå'}\")\n",
        "        print(f\"üìù Transcript file: {'‚úÖ' if transcript_ok else '‚ùå'}\")\n",
        "        print(f\"üîë API Key: {'‚úÖ' if api_ok else '‚ùå'}\")\n",
        "\n",
        "        if codebook_ok and transcript_ok and api_ok:\n",
        "            print(\"\\n‚úÖ All tests passed! Ready to start analysis.\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Please address the issues above before proceeding.\")\n",
        "\n",
        "def load_files(b):\n",
        "    with status_output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        try:\n",
        "            # Load codebook\n",
        "            if codebook_upload.value:\n",
        "                content = list(codebook_upload.value.values())[0]['content']\n",
        "                config['codebook_df'] = pd.read_csv(pd.io.common.BytesIO(content))\n",
        "                print(f\"‚úÖ Codebook loaded: {len(config['codebook_df'])} codes\")\n",
        "\n",
        "                # Validate codebook columns\n",
        "                required_cols = ['code_label', 'definition', 'inclusion_criteria',\n",
        "                               'exclusion_criteria', 'example_1', 'example_2']\n",
        "                missing_cols = [col for col in required_cols if col not in config['codebook_df'].columns]\n",
        "                if missing_cols:\n",
        "                    print(f\"‚ö†Ô∏è Warning: Missing columns in codebook: {missing_cols}\")\n",
        "            else:\n",
        "                print(\"‚ùå Please upload a codebook CSV file\")\n",
        "                return\n",
        "\n",
        "            # Load transcripts\n",
        "            if transcript_upload.value:\n",
        "                content = list(transcript_upload.value.values())[0]['content']\n",
        "                config['transcript_df'] = pd.read_csv(pd.io.common.BytesIO(content))\n",
        "                print(f\"‚úÖ Transcripts loaded: {len(config['transcript_df'])} chunks\")\n",
        "\n",
        "                # Validate transcript columns\n",
        "                if 'chunk_id' not in config['transcript_df'].columns or 'text' not in config['transcript_df'].columns:\n",
        "                    print(\"‚ùå Error: Transcript file must have 'chunk_id' and 'text' columns\")\n",
        "                    return\n",
        "            else:\n",
        "                print(\"‚ùå Please upload a transcript CSV file\")\n",
        "                return\n",
        "\n",
        "            # Update configuration\n",
        "            config['coding_approach'] = coding_approach.value.lower()\n",
        "            config['ai_model'] = ai_model_dropdown.value\n",
        "            config['api_key'] = api_key_input.value\n",
        "\n",
        "            # Validate API key\n",
        "            if not config['api_key']:\n",
        "                print(\"‚ùå Please enter your Anthropic API key\")\n",
        "                return\n",
        "\n",
        "            print(\"\\n‚úÖ Configuration complete! Ready to start analysis.\")\n",
        "            print(f\"\\nüìÅ Output folder: {config['output_folder']}\")\n",
        "            print(f\"üîß Coding approach: {config['coding_approach']}\")\n",
        "            print(f\"üß† AI Model: {ai_model_dropdown.label}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading files: {e}\")\n",
        "\n",
        "test_button.on_click(test_setup)\n",
        "process_button.on_click(load_files)\n",
        "\n",
        "# Header section with title and how-to-use\n",
        "header_section = widgets.HTML(\"\"\"\n",
        "<div style=\"background-color: #E7ECEF; padding: 20px; border-radius: 10px; border-left: 5px solid #274C77; margin-bottom: 20px;\">\n",
        "    <h2 style=\"color: #274C77; margin: 0 0 10px 0;\">üéØ Qualitative Coding Analyzer</h2>\n",
        "    <p><strong>Welcome!</strong> This tool helps social scientists systematically apply codes to interview transcripts and qualitative data using AI-powered analysis.</p>\n",
        "\n",
        "    <h4 style=\"color: #274C77; margin: 15px 0 10px 0;\">üìã How to Use:</h4>\n",
        "    <ol style=\"margin: 0; padding-left: 20px;\">\n",
        "        <li><strong>Configure:</strong> Upload your files and adjust analysis settings below</li>\n",
        "        <li><strong>Upload:</strong> Add your codebook and chunked transcript CSV files</li>\n",
        "        <li><strong>Process:</strong> Run the AI-powered coding analysis</li>\n",
        "        <li><strong>Export:</strong> Download your coded data with applied qualitative codes</li>\n",
        "    </ol>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "# Configuration guide section\n",
        "guide_section = widgets.HTML(\"\"\"\n",
        "<div style=\"background-color: #A3CEF1; padding: 20px; border-radius: 10px; margin-bottom: 20px;\">\n",
        "    <h4 style=\"color: #274C77; margin: 0 0 15px 0;\">üìö Configuration Guide</h4>\n",
        "\n",
        "    <div style=\"display: flex; gap: 20px; flex-wrap: wrap;\">\n",
        "        <div style=\"flex: 1; min-width: 300px;\">\n",
        "            <strong>‚Ä¢ Coding Approaches:</strong>\n",
        "            <ul style=\"margin: 5px 0 15px 20px;\">\n",
        "                <li><strong>Deductive:</strong> Apply pre-defined codes from your codebook - best when you have established theoretical frameworks</li>\n",
        "                <li><strong>Inductive:</strong> Generate new codes by discovering emergent themes - ideal for exploratory research</li>\n",
        "                <li><strong>Hybrid:</strong> Combine both approaches to apply existing codes and discover new themes (recommended)</li>\n",
        "            </ul>\n",
        "\n",
        "            <strong>‚Ä¢ AI Models:</strong>\n",
        "            <ul style=\"margin: 5px 0 15px 20px;\">\n",
        "                <li><strong>Claude 4 Sonnet:</strong> Latest model with enhanced reasoning capabilities (recommended)</li>\n",
        "                <li><strong>Claude 3.5 Sonnet:</strong> Proven performance with excellent qualitative analysis</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "\n",
        "        <div style=\"flex: 1; min-width: 300px;\">\n",
        "            <strong>‚Ä¢ Input File Requirements:</strong>\n",
        "            <ul style=\"margin: 5px 0 15px 20px;\">\n",
        "                <li><strong>Codebook CSV:</strong> Input file from Codebook Generator notebook (or manual creation) with code_label, definition, inclusion_criteria, exclusion_criteria, example_1, example_2 columns</li>\n",
        "                <li><strong>Transcript CSV:</strong> Input file from Interview Transcript Semantic Chunker notebook with chunk_id and text columns</li>\n",
        "                <li><strong>Format:</strong> UTF-8 encoded CSV files with proper headers</li>\n",
        "            </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "# File uploads section with actions\n",
        "file_uploads_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #274C77;'>üìÅ File Uploads</h4>\"),\n",
        "    codebook_upload,\n",
        "    transcript_upload,\n",
        "    widgets.HTML(\"<br>\"),\n",
        "    widgets.HTML(\"<h4 style='color: #274C77;'>üöÄ Actions</h4>\"),\n",
        "    widgets.HBox([process_button, test_button], layout=widgets.Layout(gap='10px'))\n",
        "])\n",
        "\n",
        "# Coding approach section\n",
        "coding_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #274C77;'>üîç Coding Approach</h4>\"),\n",
        "    coding_approach\n",
        "])\n",
        "\n",
        "# AI Configuration section\n",
        "ai_section = widgets.VBox([\n",
        "    widgets.HTML(\"<h4 style='color: #274C77;'>üß† AI Configuration</h4>\"),\n",
        "    ai_model_dropdown,\n",
        "    widgets.HTML(\"<br>\"),\n",
        "    api_key_input\n",
        "])\n",
        "\n",
        "# Main configuration container - three columns in one row with even spacing\n",
        "main_config = widgets.HBox([\n",
        "    file_uploads_section,\n",
        "    coding_section,\n",
        "    ai_section\n",
        "], layout=widgets.Layout(justify_content='space-between', width='75%'))\n",
        "\n",
        "# Display the complete interface\n",
        "display(widgets.VBox([\n",
        "    header_section,\n",
        "    guide_section,\n",
        "    main_config,\n",
        "    status_output\n",
        "]))"
      ],
      "metadata": {
        "id": "xXNAlwGWEKIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Analysis Classes\n",
        "\n",
        "Define the core classes for deductive coding, AI integration, and analysis functions that power the coding process."
      ],
      "metadata": {
        "id": "9ap2KRJ0EnTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DeductiveCoder:\n",
        "    \"\"\"\n",
        "    Handles deductive coding using a predefined codebook.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, codebook_df):\n",
        "        self.codebook_df = codebook_df\n",
        "        self.code_dict = self._build_code_dict()\n",
        "        self.coding_history = []\n",
        "\n",
        "    def _build_code_dict(self):\n",
        "        \"\"\"Build a dictionary structure from the codebook.\"\"\"\n",
        "        code_dict = {}\n",
        "\n",
        "        for _, row in self.codebook_df.iterrows():\n",
        "            code = row['code_label']\n",
        "            code_dict[code] = {\n",
        "                'definition': row['definition'],\n",
        "                'inclusion': row.get('inclusion_criteria', ''),\n",
        "                'exclusion': row.get('exclusion_criteria', ''),\n",
        "                'examples': [row.get('example_1', ''), row.get('example_2', '')]\n",
        "            }\n",
        "\n",
        "        return code_dict\n",
        "\n",
        "    def display_codebook_summary(self):\n",
        "        \"\"\"Display an organized summary of the codebook.\"\"\"\n",
        "        print(\"\\nüìö CODEBOOK REFERENCE\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        for code, details in self.code_dict.items():\n",
        "            print(f\"\\nüè∑Ô∏è  {code}\")\n",
        "            print(f\"   Definition: {details['definition']}\")\n",
        "            if details['inclusion']:\n",
        "                print(f\"   ‚úì Include: {details['inclusion']}\")\n",
        "            if details['exclusion']:\n",
        "                print(f\"   ‚úó Exclude: {details['exclusion']}\")\n",
        "\n",
        "    def validate_codes(self, codes_list):\n",
        "        \"\"\"Validate that provided codes exist in the codebook.\"\"\"\n",
        "        valid_codes = []\n",
        "        invalid_codes = []\n",
        "\n",
        "        all_codes = list(self.code_dict.keys())\n",
        "\n",
        "        for code in codes_list:\n",
        "            if code.strip() in all_codes:\n",
        "                valid_codes.append(code.strip())\n",
        "            else:\n",
        "                invalid_codes.append(code.strip())\n",
        "\n",
        "        return {\n",
        "            'valid': valid_codes,\n",
        "            'invalid': invalid_codes,\n",
        "            'all_valid': len(invalid_codes) == 0\n",
        "        }\n",
        "\n",
        "\n",
        "class ClaudeAutoCoder:\n",
        "    \"\"\"\n",
        "    Automated coding using Claude API or alternative methods.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, codebook_df: pd.DataFrame, coder: DeductiveCoder, use_ai: bool = True):\n",
        "        self.use_ai = use_ai\n",
        "        self.codebook_df = codebook_df\n",
        "        self.coder = coder\n",
        "        self.coding_history = []\n",
        "\n",
        "        if use_ai:\n",
        "            self.client = anthropic.Anthropic(api_key=api_key)\n",
        "            self.coding_prompt = self._build_coding_prompt()\n",
        "        else:\n",
        "            self.client = None\n",
        "            self.coding_prompt = None\n",
        "\n",
        "    def _build_coding_prompt(self) -> str:\n",
        "        \"\"\"Build the system prompt for Claude with the complete codebook.\"\"\"\n",
        "        codebook_text = \"DEDUCTIVE CODING CODEBOOK:\\n\\n\"\n",
        "\n",
        "        for code, details in self.coder.code_dict.items():\n",
        "            codebook_text += f\"CODE: {code}\\n\"\n",
        "            codebook_text += f\"Definition: {details['definition']}\\n\"\n",
        "            if details['inclusion']:\n",
        "                codebook_text += f\"Include when: {details['inclusion']}\\n\"\n",
        "            if details['exclusion']:\n",
        "                codebook_text += f\"Exclude when: {details['exclusion']}\\n\"\n",
        "            codebook_text += \"\\n\"\n",
        "\n",
        "        prompt = f\"\"\"You are a qualitative research assistant specializing in deductive coding. Your task is to analyze text segments and identify which codes from the codebook apply.\n",
        "\n",
        "{codebook_text}\n",
        "\n",
        "CODING INSTRUCTIONS:\n",
        "1. Read each text segment carefully\n",
        "2. Apply ALL relevant codes from the codebook\n",
        "3. Only use codes that are explicitly defined above\n",
        "4. Return codes as a comma-separated list (e.g., \"CODE1,CODE2,CODE3\")\n",
        "5. If no codes apply, return \"NO_CODES\"\n",
        "6. Be consistent - similar content should receive similar codes\n",
        "7. Focus on manifest content (what is explicitly stated)\n",
        "\n",
        "Return only the comma-separated codes, no explanation needed.\"\"\"\n",
        "\n",
        "        return prompt\n",
        "\n",
        "    def code_single_chunk(self, text: str, chunk_id: str = None) -> Dict:\n",
        "        \"\"\"Code a single text chunk.\"\"\"\n",
        "        if pd.isna(text) or str(text).strip() == '':\n",
        "            return {\n",
        "                'chunk_id': chunk_id,\n",
        "                'codes': '',\n",
        "                'valid': True,\n",
        "                'error': None\n",
        "            }\n",
        "\n",
        "        if self.use_ai:\n",
        "            return self._code_with_ai(text, chunk_id)\n",
        "        else:\n",
        "            return self._code_without_ai(text, chunk_id)\n",
        "\n",
        "    def _code_with_ai(self, text: str, chunk_id: str) -> Dict:\n",
        "        \"\"\"Code using Claude API.\"\"\"\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=config['ai_model'],\n",
        "                max_tokens=150,\n",
        "                temperature=0.1,\n",
        "                system=self.coding_prompt,\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Code this text: {text}\"\n",
        "                }]\n",
        "            )\n",
        "\n",
        "            raw_codes = response.content[0].text.strip()\n",
        "\n",
        "            if raw_codes.upper() == \"NO_CODES\":\n",
        "                return {\n",
        "                    'chunk_id': chunk_id,\n",
        "                    'codes': '',\n",
        "                    'valid': True,\n",
        "                    'error': None\n",
        "                }\n",
        "\n",
        "            # Validate codes\n",
        "            codes_list = [code.strip() for code in raw_codes.split(',')]\n",
        "            validation = self.coder.validate_codes(codes_list)\n",
        "\n",
        "            return {\n",
        "                'chunk_id': chunk_id,\n",
        "                'codes': ','.join(validation['valid']),\n",
        "                'valid': validation['all_valid'],\n",
        "                'error': f\"Invalid codes removed: {validation['invalid']}\" if validation['invalid'] else None\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'chunk_id': chunk_id,\n",
        "                'codes': '',\n",
        "                'valid': False,\n",
        "                'error': f\"API Error: {str(e)}\"\n",
        "            }\n",
        "\n",
        "    def _code_without_ai(self, text: str, chunk_id: str) -> Dict:\n",
        "        \"\"\"Simple keyword matching for non-AI coding.\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        matched_codes = []\n",
        "\n",
        "        for code, details in self.coder.code_dict.items():\n",
        "            # Simple keyword matching from definition and examples\n",
        "            keywords = []\n",
        "            keywords.extend(details['definition'].lower().split())\n",
        "            keywords.extend([ex.lower() for ex in details['examples'] if ex])\n",
        "\n",
        "            # Check if any keyword appears in text\n",
        "            if any(keyword in text_lower for keyword in keywords if len(keyword) > 3):\n",
        "                matched_codes.append(code)\n",
        "\n",
        "        return {\n",
        "            'chunk_id': chunk_id,\n",
        "            'codes': ','.join(matched_codes),\n",
        "            'valid': True,\n",
        "            'error': None\n",
        "        }\n",
        "\n",
        "    def code_batch(self, df: pd.DataFrame, text_column: str = 'text',\n",
        "                   chunk_id_column: str = 'chunk_id', delay_seconds: float = 1.0) -> pd.DataFrame:\n",
        "        \"\"\"Code a batch of text chunks.\"\"\"\n",
        "        total_chunks = len(df)\n",
        "\n",
        "        print(f\"\\nü§ñ Starting Deductive Coding\")\n",
        "        print(f\"Processing {total_chunks} text chunks\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Add columns for deductive coding\n",
        "        if 'Deductive_Codes' not in df.columns:\n",
        "            df['Deductive_Codes'] = ''\n",
        "        if 'Coding_Status' not in df.columns:\n",
        "            df['Coding_Status'] = ''\n",
        "\n",
        "        successful_codes = 0\n",
        "        failed_codes = 0\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            text = row[text_column]\n",
        "            chunk_id = row[chunk_id_column]\n",
        "\n",
        "            print(f\"Coding chunk {idx+1}/{total_chunks} ({((idx+1)/total_chunks*100):.1f}%)\", end=\" \")\n",
        "\n",
        "            result = self.code_single_chunk(text, chunk_id)\n",
        "\n",
        "            # Update DataFrame\n",
        "            df.at[idx, 'Deductive_Codes'] = result['codes']\n",
        "            df.at[idx, 'Coding_Status'] = 'Deductive_Coded' if result['codes'] else 'No_Deductive_Codes'\n",
        "\n",
        "            if result['valid'] and not result['error']:\n",
        "                successful_codes += 1\n",
        "                print(\"‚úÖ\")\n",
        "            else:\n",
        "                failed_codes += 1\n",
        "                print(f\"‚ö†Ô∏è  {result['error']}\")\n",
        "\n",
        "            if self.use_ai and idx < total_chunks - 1:\n",
        "                time.sleep(delay_seconds)\n",
        "\n",
        "        print(f\"\\n‚úÖ Deductive coding completed!\")\n",
        "        print(f\"Successfully coded: {successful_codes}\")\n",
        "        print(f\"Failed/partial: {failed_codes}\")\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "3sVz2hCyEs6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inductive Coding Classes\n",
        "\n",
        "Define classes for discovering emergent themes and patterns not captured in the deductive codebook."
      ],
      "metadata": {
        "id": "xic3Cmt6Ev6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClaudeInductiveCoder:\n",
        "    \"\"\"\n",
        "    Use Claude API for inductive coding to discover emergent themes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, claude_coder: ClaudeAutoCoder, coded_df: pd.DataFrame):\n",
        "        self.claude_coder = claude_coder\n",
        "        self.coded_df = coded_df\n",
        "        self.client = claude_coder.client\n",
        "        self.use_ai = claude_coder.use_ai\n",
        "        self.discovered_codes = {}\n",
        "\n",
        "    def generate_inductive_codes(self, sample_size: int = 50) -> Dict:\n",
        "        \"\"\"Discover emergent codes from the data.\"\"\"\n",
        "        print(f\"\\nüîç GENERATING INDUCTIVE CODES\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        # Sample chunks for analysis\n",
        "        all_chunks = self.coded_df[self.coded_df['text'].notna()]\n",
        "        sample_chunks = all_chunks.sample(min(sample_size, len(all_chunks)))\n",
        "\n",
        "        print(f\"üìä Analyzing {len(sample_chunks)} chunks for emergent patterns...\")\n",
        "\n",
        "        if self.use_ai:\n",
        "            return self._generate_with_ai(sample_chunks)\n",
        "        else:\n",
        "            return self._generate_without_ai(sample_chunks)\n",
        "\n",
        "    def _generate_with_ai(self, sample_chunks: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate inductive codes using AI.\"\"\"\n",
        "        # Prepare chunks with their deductive codes for context\n",
        "        chunks_text = \"\"\n",
        "        for idx, row in sample_chunks.iterrows():\n",
        "            text = str(row['text'])[:400] + \"...\" if len(str(row['text'])) > 400 else str(row['text'])\n",
        "            deductive_codes = row.get('Deductive_Codes', '')\n",
        "            chunks_text += f\"\\nChunk {row['chunk_id']}:\\n{text}\\nDeductive codes: {deductive_codes}\\n---\\n\"\n",
        "\n",
        "        inductive_prompt = f\"\"\"You are conducting INDUCTIVE CODING on interview transcripts.\n",
        "Your task is to identify EMERGENT THEMES that are NOT captured by the existing deductive codes.\n",
        "\n",
        "SAMPLE CHUNKS FOR ANALYSIS:\n",
        "{chunks_text}\n",
        "\n",
        "TASK: Identify 8-12 EMERGENT INDUCTIVE CODES that capture important patterns NOT covered by deductive codes.\n",
        "\n",
        "For each code provide:\n",
        "**INDUCTIVE CODE: [SHORT_NAME]**\n",
        "Definition: [Clear description]\n",
        "Rationale: [Why this is important]\n",
        "Example: \"[Direct quote]\"\n",
        "When to Apply: [Clear criteria]\n",
        "\n",
        "Ensure there is a blank line between codes.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=config['ai_model'],\n",
        "                max_tokens=3000,\n",
        "                temperature=0.4,\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": inductive_prompt\n",
        "                }]\n",
        "            )\n",
        "\n",
        "            inductive_analysis = response.content[0].text\n",
        "            self.discovered_codes = self._parse_inductive_codes(inductive_analysis)\n",
        "\n",
        "            print(f\"‚úÖ Found {len(self.discovered_codes)} inductive codes\")\n",
        "\n",
        "            return {\n",
        "                'inductive_analysis': inductive_analysis,\n",
        "                'discovered_codes': self.discovered_codes,\n",
        "                'sample_size': len(sample_chunks)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in inductive analysis: {e}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def _generate_without_ai(self, sample_chunks: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Generate inductive codes using frequency analysis.\"\"\"\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.cluster import KMeans\n",
        "\n",
        "        # Simple TF-IDF based theme discovery\n",
        "        texts = sample_chunks['text'].fillna('').tolist()\n",
        "\n",
        "        try:\n",
        "            vectorizer = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 3))\n",
        "            tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "            # Get top terms\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "            # Simple clustering\n",
        "            n_clusters = min(8, len(texts) // 5)\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "            clusters = kmeans.fit_predict(tfidf_matrix)\n",
        "\n",
        "            # Generate codes from clusters\n",
        "            for i in range(n_clusters):\n",
        "                cluster_terms = []\n",
        "                cluster_docs = [texts[j] for j in range(len(texts)) if clusters[j] == i]\n",
        "\n",
        "                if cluster_docs:\n",
        "                    # Get representative terms\n",
        "                    cluster_vectorizer = TfidfVectorizer(max_features=5, stop_words='english')\n",
        "                    cluster_tfidf = cluster_vectorizer.fit_transform(cluster_docs)\n",
        "                    terms = cluster_vectorizer.get_feature_names_out()\n",
        "\n",
        "                    code_name = f\"THEME_{i+1}_{terms[0].upper()}\"\n",
        "                    self.discovered_codes[code_name] = {\n",
        "                        'definition': f\"Theme related to {', '.join(terms[:3])}\",\n",
        "                        'rationale': \"Emerged from clustering analysis\",\n",
        "                        'example': cluster_docs[0][:100] + \"...\",\n",
        "                        'application': f\"Apply when text mentions {terms[0]}\"\n",
        "                    }\n",
        "\n",
        "            print(f\"‚úÖ Found {len(self.discovered_codes)} inductive themes through clustering\")\n",
        "\n",
        "            return {\n",
        "                'discovered_codes': self.discovered_codes,\n",
        "                'sample_size': len(sample_chunks)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error in clustering: {e}\")\n",
        "            # Fallback to simple frequency analysis\n",
        "            return self._simple_frequency_codes(sample_chunks)\n",
        "\n",
        "    def _simple_frequency_codes(self, sample_chunks: pd.DataFrame) -> Dict:\n",
        "        \"\"\"Fallback method using simple frequency analysis.\"\"\"\n",
        "        from collections import Counter\n",
        "        import re\n",
        "\n",
        "        # Extract common phrases\n",
        "        all_text = ' '.join(sample_chunks['text'].fillna('').tolist()).lower()\n",
        "        words = re.findall(r'\\b\\w+\\b', all_text)\n",
        "\n",
        "        # Filter common words\n",
        "        stop_words = {'the', 'is', 'at', 'which', 'on', 'and', 'a', 'an', 'as', 'are', 'was', 'were', 'i', 'you', 'he', 'she', 'it', 'they', 'we'}\n",
        "        words = [w for w in words if w not in stop_words and len(w) > 3]\n",
        "\n",
        "        # Get most common words\n",
        "        word_freq = Counter(words).most_common(20)\n",
        "\n",
        "        # Create simple codes\n",
        "        for i, (word, freq) in enumerate(word_freq[:8]):\n",
        "            code_name = f\"FREQ_{word.upper()}\"\n",
        "            self.discovered_codes[code_name] = {\n",
        "                'definition': f\"References to {word} (appeared {freq} times)\",\n",
        "                'rationale': \"High frequency term in the data\",\n",
        "                'example': f\"Text containing '{word}'\",\n",
        "                'application': f\"Apply when text mentions {word}\"\n",
        "            }\n",
        "\n",
        "        print(f\"‚úÖ Found {len(self.discovered_codes)} frequency-based codes\")\n",
        "\n",
        "        return {\n",
        "            'discovered_codes': self.discovered_codes,\n",
        "            'sample_size': len(sample_chunks)\n",
        "        }\n",
        "\n",
        "    def _parse_inductive_codes(self, analysis_text):\n",
        "        \"\"\"Parse inductive codes from Claude's analysis.\"\"\"\n",
        "        codes = {}\n",
        "\n",
        "        if not analysis_text:\n",
        "            return codes\n",
        "\n",
        "        # Use regex to find each code block\n",
        "        code_blocks = re.split(r'\\*\\*INDUCTIVE CODE: (.+?)\\*\\*\\s*\\n', analysis_text, flags=re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if len(code_blocks) < 2:\n",
        "            return codes\n",
        "\n",
        "        for i in range(1, len(code_blocks), 2):\n",
        "            code_name = code_blocks[i].strip()\n",
        "            block_content = code_blocks[i+1].strip() if i+1 < len(code_blocks) else ''\n",
        "\n",
        "            codes[code_name] = {\n",
        "                'definition': '',\n",
        "                'rationale': '',\n",
        "                'example': '',\n",
        "                'application': ''\n",
        "            }\n",
        "\n",
        "            # Extract fields\n",
        "            definition_match = re.search(r'Definition: (.+?)(?:\\nRationale:|\\nExample:|\\nWhen to Apply:|$)', block_content, re.DOTALL)\n",
        "            if definition_match:\n",
        "                codes[code_name]['definition'] = definition_match.group(1).strip()\n",
        "\n",
        "            rationale_match = re.search(r'Rationale: (.+?)(?:\\nDefinition:|\\nExample:|\\nWhen to Apply:|$)', block_content, re.DOTALL)\n",
        "            if rationale_match:\n",
        "                codes[code_name]['rationale'] = rationale_match.group(1).strip()\n",
        "\n",
        "            example_match = re.search(r'Example: (.+?)(?:\\nDefinition:|\\nRationale:|\\nWhen to Apply:|$)', block_content, re.DOTALL)\n",
        "            if example_match:\n",
        "                codes[code_name]['example'] = example_match.group(1).strip()\n",
        "\n",
        "            application_match = re.search(r'When to Apply: (.+?)(?:\\nDefinition:|\\nRationale:|\\nExample:|$)', block_content, re.DOTALL)\n",
        "            if application_match:\n",
        "                codes[code_name]['application'] = application_match.group(1).strip()\n",
        "\n",
        "        return codes\n",
        "\n",
        "    def apply_inductive_codes(self, delay_seconds: float = 1.0) -> pd.DataFrame:\n",
        "        \"\"\"Apply discovered inductive codes to all chunks.\"\"\"\n",
        "        print(f\"\\nüìù APPLYING INDUCTIVE CODES\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        if not self.discovered_codes:\n",
        "            print(\"‚ùå No inductive codes to apply.\")\n",
        "            return self.coded_df\n",
        "\n",
        "        # Add inductive coding columns\n",
        "        if 'Inductive_Codes' not in self.coded_df.columns:\n",
        "            self.coded_df['Inductive_Codes'] = ''\n",
        "\n",
        "        total_chunks = len(self.coded_df)\n",
        "        successful = 0\n",
        "\n",
        "        if self.use_ai:\n",
        "            # Create prompt for applying codes\n",
        "            codes_text = \"\\n\".join([\n",
        "                f\"{code}: {details['definition']} (Apply when: {details['application']})\"\n",
        "                for code, details in self.discovered_codes.items()\n",
        "            ])\n",
        "\n",
        "            apply_prompt = f\"\"\"Apply these INDUCTIVE CODES to text chunks.\n",
        "\n",
        "INDUCTIVE CODES:\n",
        "{codes_text}\n",
        "\n",
        "Instructions:\n",
        "1. Apply ONLY codes that clearly match\n",
        "2. Return codes as comma-separated list\n",
        "3. If no codes apply, return \"NONE\"\n",
        "\n",
        "Return ONLY the code names.\"\"\"\n",
        "\n",
        "            for idx, row in self.coded_df.iterrows():\n",
        "                if pd.notna(row['text']):\n",
        "                    text = str(row['text'])\n",
        "\n",
        "                    print(f\"Applying inductive codes {idx+1}/{total_chunks} ({(idx+1)/total_chunks*100:.1f}%)\", end=\" \")\n",
        "\n",
        "                    try:\n",
        "                        response = self.client.messages.create(\n",
        "                            model=config['ai_model'],\n",
        "                            max_tokens=100,\n",
        "                            temperature=0.1,\n",
        "                            system=apply_prompt,\n",
        "                            messages=[{\n",
        "                                \"role\": \"user\",\n",
        "                                \"content\": f\"Text: {text[:800]}\"\n",
        "                            }]\n",
        "                        )\n",
        "\n",
        "                        result = response.content[0].text.strip()\n",
        "\n",
        "                        if result != \"NONE\":\n",
        "                            self.coded_df.at[idx, 'Inductive_Codes'] = result\n",
        "                            successful += 1\n",
        "                            print(\"‚úÖ\")\n",
        "                        else:\n",
        "                            print(\"‚≠ï\")\n",
        "\n",
        "                        time.sleep(delay_seconds)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå Error: {e}\")\n",
        "                        continue\n",
        "        else:\n",
        "            # Apply codes using keyword matching\n",
        "            for idx, row in self.coded_df.iterrows():\n",
        "                if pd.notna(row['text']):\n",
        "                    text = str(row['text']).lower()\n",
        "                    matched_codes = []\n",
        "\n",
        "                    for code, details in self.discovered_codes.items():\n",
        "                        # Simple keyword matching\n",
        "                        keywords = details['definition'].lower().split()\n",
        "                        if any(keyword in text for keyword in keywords if len(keyword) > 3):\n",
        "                            matched_codes.append(code)\n",
        "\n",
        "                    if matched_codes:\n",
        "                        self.coded_df.at[idx, 'Inductive_Codes'] = ','.join(matched_codes)\n",
        "                        successful += 1\n",
        "\n",
        "        print(f\"\\n‚úÖ Applied inductive codes to {successful} chunks\")\n",
        "\n",
        "        # Update coding status\n",
        "        for idx, row in self.coded_df.iterrows():\n",
        "            has_deductive = pd.notna(row.get('Deductive_Codes', '')) and row.get('Deductive_Codes', '') != ''\n",
        "            has_inductive = pd.notna(row.get('Inductive_Codes', '')) and row.get('Inductive_Codes', '') != ''\n",
        "\n",
        "            if has_deductive and has_inductive:\n",
        "                self.coded_df.at[idx, 'Coding_Status'] = 'Both_Deductive_Inductive'\n",
        "            elif has_deductive:\n",
        "                self.coded_df.at[idx, 'Coding_Status'] = 'Deductive_Only'\n",
        "            elif has_inductive:\n",
        "                self.coded_df.at[idx, 'Coding_Status'] = 'Inductive_Only'\n",
        "            else:\n",
        "                self.coded_df.at[idx, 'Coding_Status'] = 'No_Codes'\n",
        "\n",
        "        return self.coded_df"
      ],
      "metadata": {
        "id": "bFvyrkEZEz8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis and Theme Building Functions\n",
        "\n",
        "Functions to analyze code patterns, build themes, and generate insights from the coded data."
      ],
      "metadata": {
        "id": "3PSy9o4iPTe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def integrate_coding_approaches(df_coded):\n",
        "    \"\"\"\n",
        "    Create integrated view of all codes for theme building.\n",
        "    \"\"\"\n",
        "    print(\"\\nüîó INTEGRATING CODING APPROACHES\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Add integration columns\n",
        "    df_coded['All_Codes'] = ''\n",
        "    df_coded['Total_Code_Count'] = 0\n",
        "\n",
        "    for idx, row in df_coded.iterrows():\n",
        "        all_codes = []\n",
        "\n",
        "        # Add deductive codes\n",
        "        if pd.notna(row.get('Deductive_Codes', '')) and row.get('Deductive_Codes', ''):\n",
        "            deductive_codes = [c.strip() for c in str(row['Deductive_Codes']).split(',')]\n",
        "            all_codes.extend(deductive_codes)\n",
        "\n",
        "        # Add inductive codes (marked with _IND suffix)\n",
        "        if pd.notna(row.get('Inductive_Codes', '')) and row.get('Inductive_Codes', ''):\n",
        "            inductive_codes = [c.strip() + '_IND' for c in str(row['Inductive_Codes']).split(',')]\n",
        "            all_codes.extend(inductive_codes)\n",
        "\n",
        "        # Update integrated columns\n",
        "        if all_codes:\n",
        "            df_coded.at[idx, 'All_Codes'] = ', '.join(all_codes)\n",
        "            df_coded.at[idx, 'Total_Code_Count'] = len(all_codes)\n",
        "\n",
        "    # Analysis of integration\n",
        "    integration_stats = {\n",
        "        'total_chunks': len(df_coded),\n",
        "        'coded_chunks': len(df_coded[df_coded['Total_Code_Count'] > 0]),\n",
        "        'deductive_only': len(df_coded[df_coded['Coding_Status'] == 'Deductive_Only']),\n",
        "        'inductive_only': len(df_coded[df_coded['Coding_Status'] == 'Inductive_Only']),\n",
        "        'both_types': len(df_coded[df_coded['Coding_Status'] == 'Both_Deductive_Inductive']),\n",
        "        'no_codes': len(df_coded[df_coded['Coding_Status'] == 'No_Codes'])\n",
        "    }\n",
        "\n",
        "    print(\"\\n‚úÖ Integration Complete:\")\n",
        "    print(f\"‚Ä¢ Total chunks: {integration_stats['total_chunks']}\")\n",
        "    print(f\"‚Ä¢ Coded chunks: {integration_stats['coded_chunks']}\")\n",
        "    print(f\"‚Ä¢ Deductive only: {integration_stats['deductive_only']}\")\n",
        "    print(f\"‚Ä¢ Inductive only: {integration_stats['inductive_only']}\")\n",
        "    print(f\"‚Ä¢ Both types: {integration_stats['both_types']}\")\n",
        "    print(f\"‚Ä¢ No codes: {integration_stats['no_codes']}\")\n",
        "\n",
        "    return df_coded, integration_stats\n",
        "\n",
        "\n",
        "def analyze_code_patterns(df_coded):\n",
        "    \"\"\"\n",
        "    Analyze patterns in the integrated codes.\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä ANALYZING CODE PATTERNS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Collect all codes\n",
        "    all_codes_list = []\n",
        "    deductive_codes_list = []\n",
        "    inductive_codes_list = []\n",
        "    code_combinations = defaultdict(int)\n",
        "\n",
        "    for idx, row in df_coded.iterrows():\n",
        "        if pd.notna(row['All_Codes']) and row['All_Codes']:\n",
        "            codes = [c.strip() for c in str(row['All_Codes']).split(',')]\n",
        "            all_codes_list.extend(codes)\n",
        "\n",
        "            # Separate deductive and inductive\n",
        "            deductive = [c for c in codes if not c.endswith('_IND')]\n",
        "            inductive = [c for c in codes if c.endswith('_IND')]\n",
        "\n",
        "            deductive_codes_list.extend(deductive)\n",
        "            inductive_codes_list.extend(inductive)\n",
        "\n",
        "            # Track combinations\n",
        "            if len(codes) > 1:\n",
        "                code_combinations[tuple(sorted(codes))] += 1\n",
        "\n",
        "    # Calculate frequencies\n",
        "    all_freq = Counter(all_codes_list)\n",
        "    deductive_freq = Counter(deductive_codes_list)\n",
        "    inductive_freq = Counter(inductive_codes_list)\n",
        "\n",
        "    patterns = {\n",
        "        'total_code_applications': len(all_codes_list),\n",
        "        'unique_codes': len(set(all_codes_list)),\n",
        "        'all_codes_frequency': dict(all_freq.most_common(20)),\n",
        "        'deductive_frequency': dict(deductive_freq.most_common(15)),\n",
        "        'inductive_frequency': dict(inductive_freq.most_common(15)),\n",
        "        'frequent_combinations': dict(sorted(code_combinations.items(), key=lambda x: x[1], reverse=True)[:15])\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìà Pattern Analysis Results:\")\n",
        "    print(f\"‚Ä¢ Total code applications: {patterns['total_code_applications']}\")\n",
        "    print(f\"‚Ä¢ Unique codes used: {patterns['unique_codes']}\")\n",
        "\n",
        "    print(f\"\\nüèÜ Top 10 Most Frequent Codes:\")\n",
        "    for code, freq in list(patterns['all_codes_frequency'].items())[:10]:\n",
        "        code_type = \"[IND]\" if code.endswith('_IND') else \"[DED]\"\n",
        "        print(f\"  ‚Ä¢ {code} {code_type}: {freq} occurrences\")\n",
        "\n",
        "    return patterns\n",
        "\n",
        "\n",
        "class IntegratedThemeBuilder:\n",
        "    \"\"\"\n",
        "    Build themes from integrated deductive and inductive codes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, claude_coder, coded_df, code_patterns, inductive_results):\n",
        "        self.claude_coder = claude_coder\n",
        "        self.coded_df = coded_df\n",
        "        self.client = claude_coder.client if claude_coder else None\n",
        "        self.use_ai = claude_coder.use_ai if claude_coder else False\n",
        "        self.code_patterns = code_patterns\n",
        "        self.inductive_results = inductive_results\n",
        "\n",
        "    def build_themes(self):\n",
        "        \"\"\"Build hierarchical themes from the integrated analysis.\"\"\"\n",
        "        print(\"\\nüéØ BUILDING THEMES FROM INTEGRATED CODES\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        if self.use_ai:\n",
        "            return self._build_themes_with_ai()\n",
        "        else:\n",
        "            return self._build_themes_without_ai()\n",
        "\n",
        "    def _build_themes_with_ai(self):\n",
        "        \"\"\"Build themes using AI.\"\"\"\n",
        "        # Get sample coded chunks\n",
        "        pattern_examples = self._get_pattern_examples()\n",
        "\n",
        "        theme_prompt = f\"\"\"You are a qualitative research expert building THEMES from mixed-method coding results.\n",
        "\n",
        "CODING OVERVIEW:\n",
        "- Total code applications: {self.code_patterns['total_code_applications']}\n",
        "- Unique codes: {self.code_patterns['unique_codes']}\n",
        "\n",
        "TOP DEDUCTIVE CODES:\n",
        "{self._format_top_codes(self.code_patterns['deductive_frequency'], 10)}\n",
        "\n",
        "TOP INDUCTIVE CODES:\n",
        "{self._format_top_codes(self.code_patterns['inductive_frequency'], 10)}\n",
        "\n",
        "SAMPLE CODED CHUNKS:\n",
        "{pattern_examples}\n",
        "\n",
        "TASK: Create 5-7 HIERARCHICAL THEMES that:\n",
        "1. Integrate insights from both deductive and inductive codes\n",
        "2. Have clear main themes with 2-3 sub-themes each\n",
        "3. Are actionable and relevant\n",
        "\n",
        "Format each theme as:\n",
        "\n",
        "THEME [Number]: [Clear, Descriptive Name]\n",
        "Core Concept: [2-3 sentences explaining what this theme captures]\n",
        "Sub-themes:\n",
        "  a) [Sub-theme name]: [Brief description]\n",
        "  b) [Sub-theme name]: [Brief description]\n",
        "Key Finding: [The main insight this theme reveals]\n",
        "Evidence Strength: [Strong/Moderate/Emerging - based on frequency]\n",
        "\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.messages.create(\n",
        "                model=config['ai_model'],\n",
        "                max_tokens=4000,\n",
        "                temperature=0.3,\n",
        "                messages=[{\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": theme_prompt\n",
        "                }]\n",
        "            )\n",
        "\n",
        "            themes_analysis = response.content[0].text\n",
        "            print(\"‚úÖ Themes successfully built!\")\n",
        "\n",
        "            return {\n",
        "                'themes_analysis': themes_analysis,\n",
        "                'code_patterns': self.code_patterns,\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error building themes: {e}\")\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def _build_themes_without_ai(self):\n",
        "        \"\"\"Build themes using frequency analysis.\"\"\"\n",
        "        themes_text = \"THEMES GENERATED FROM FREQUENCY ANALYSIS\\n\\n\"\n",
        "\n",
        "        # Group codes by frequency\n",
        "        top_codes = list(self.code_patterns['all_codes_frequency'].items())[:15]\n",
        "\n",
        "        # Create simple themes based on most frequent codes\n",
        "        theme_num = 1\n",
        "        for i in range(0, len(top_codes), 3):\n",
        "            theme_codes = top_codes[i:i+3]\n",
        "            if theme_codes:\n",
        "                main_code = theme_codes[0][0]\n",
        "                themes_text += f\"\\nTHEME {theme_num}: {main_code.replace('_', ' ').title()}\\n\"\n",
        "                themes_text += f\"Core Concept: This theme encompasses patterns related to {main_code} \"\n",
        "                themes_text += f\"which appeared {theme_codes[0][1]} times in the data.\\n\"\n",
        "                themes_text += \"Sub-themes:\\n\"\n",
        "\n",
        "                for j, (code, freq) in enumerate(theme_codes[1:], 1):\n",
        "                    themes_text += f\"  {chr(96+j)}) {code}: Frequency: {freq}\\n\"\n",
        "\n",
        "                themes_text += f\"Key Finding: High frequency of {main_code} suggests its importance.\\n\"\n",
        "                themes_text += \"Evidence Strength: Strong (based on frequency)\\n\"\n",
        "                theme_num += 1\n",
        "\n",
        "        return {\n",
        "            'themes_analysis': themes_text,\n",
        "            'code_patterns': self.code_patterns,\n",
        "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "    def _get_pattern_examples(self):\n",
        "        \"\"\"Get example chunks for major patterns.\"\"\"\n",
        "        examples = []\n",
        "\n",
        "        # Get examples of chunks with both deductive and inductive codes\n",
        "        mixed_chunks = self.coded_df[self.coded_df['Coding_Status'] == 'Both_Deductive_Inductive'].head(3)\n",
        "\n",
        "        for idx, row in mixed_chunks.iterrows():\n",
        "            text_preview = str(row['text'])[:200] + \"...\"\n",
        "            ded_codes = row.get('Deductive_Codes', 'None')\n",
        "            ind_codes = row.get('Inductive_Codes', 'None')\n",
        "            examples.append(f\"\\nChunk {row['chunk_id']}:\\nText: {text_preview}\\nDeductive: {ded_codes}\\nInductive: {ind_codes}\")\n",
        "\n",
        "        return '\\n'.join(examples) if examples else \"No examples available\"\n",
        "\n",
        "    def _format_top_codes(self, freq_dict, limit):\n",
        "        \"\"\"Format top codes for the prompt.\"\"\"\n",
        "        lines = []\n",
        "        for code, freq in list(freq_dict.items())[:limit]:\n",
        "            lines.append(f\"‚Ä¢ {code}: {freq} occurrences\")\n",
        "        return '\\n'.join(lines)"
      ],
      "metadata": {
        "id": "ZTV_pF7eE9dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization Functions\n",
        "\n",
        "Create visualizations including network graphs, word clouds, and statistical charts to illuminate patterns in the coded data."
      ],
      "metadata": {
        "id": "65vPE8uiFBIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_comprehensive_visualizations(df_coded, code_patterns, themes, coder):\n",
        "    \"\"\"\n",
        "    Create all visualizations for the analysis.\n",
        "    \"\"\"\n",
        "    print(\"\\nüìä CREATING VISUALIZATIONS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Set color scheme\n",
        "    colors = {\n",
        "        'primary': '#274C77',\n",
        "        'secondary': '#6096BA',\n",
        "        'accent': '#A3CEF1',\n",
        "        'neutral': '#8B8C89',\n",
        "        'background': '#E7ECEF'\n",
        "    }\n",
        "\n",
        "    # 1. Coding Method Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    coding_status = df_coded['Coding_Status'].value_counts()\n",
        "    plt.pie(coding_status.values, labels=coding_status.index, autopct='%1.1f%%',\n",
        "            colors=[colors['primary'], colors['secondary'], colors['accent'], colors['neutral'], colors['background']],\n",
        "            startangle=90)\n",
        "    plt.title('Distribution of Coding Methods', fontsize=16, fontweight='bold', color=colors['primary'])\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{config['output_folder']}/coding_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Code Frequency Comparison\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # Deductive codes\n",
        "    ded_codes = list(code_patterns['deductive_frequency'].items())[:15]\n",
        "    if ded_codes:\n",
        "        codes, freqs = zip(*ded_codes)\n",
        "        ax1.barh(codes, freqs, color=colors['primary'])\n",
        "        ax1.set_xlabel('Frequency', fontsize=12)\n",
        "        ax1.set_title('Top 15 Deductive Codes', fontsize=14, fontweight='bold', color=colors['primary'])\n",
        "        ax1.invert_yaxis()\n",
        "\n",
        "    # Inductive codes\n",
        "    ind_codes = list(code_patterns['inductive_frequency'].items())[:15]\n",
        "    if ind_codes:\n",
        "        codes, freqs = zip(*ind_codes)\n",
        "        codes = [c.replace('_IND', '') for c in codes]\n",
        "        ax2.barh(codes, freqs, color=colors['secondary'])\n",
        "        ax2.set_xlabel('Frequency', fontsize=12)\n",
        "        ax2.set_title('Top 15 Inductive Codes', fontsize=14, fontweight='bold', color=colors['primary'])\n",
        "        ax2.invert_yaxis()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{config['output_folder']}/code_frequencies.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Code Co-occurrence Heatmap\n",
        "    create_code_cooccurrence_heatmap(df_coded, code_patterns, colors)\n",
        "\n",
        "    # 4. Network Graph (if needed)\n",
        "    if len(code_patterns['all_codes_frequency']) > 5:\n",
        "        create_code_network_graph(df_coded, colors)\n",
        "\n",
        "    print(\"\\n‚úÖ All visualizations created and saved!\")\n",
        "\n",
        "\n",
        "def create_code_cooccurrence_heatmap(df_coded, code_patterns, colors):\n",
        "    \"\"\"\n",
        "    Create heatmap of code co-occurrences.\n",
        "    \"\"\"\n",
        "    # Get top 20 codes overall\n",
        "    top_codes = list(code_patterns['all_codes_frequency'].keys())[:20]\n",
        "\n",
        "    # Build co-occurrence matrix\n",
        "    matrix = pd.DataFrame(0, index=top_codes, columns=top_codes)\n",
        "\n",
        "    for idx, row in df_coded.iterrows():\n",
        "        if pd.notna(row['All_Codes']) and row['All_Codes']:\n",
        "            codes = [c.strip() for c in str(row['All_Codes']).split(',')]\n",
        "            codes = [c for c in codes if c in top_codes]\n",
        "\n",
        "            for i, code1 in enumerate(codes):\n",
        "                for code2 in codes[i:]:\n",
        "                    matrix.loc[code1, code2] += 1\n",
        "                    if code1 != code2:\n",
        "                        matrix.loc[code2, code1] += 1\n",
        "\n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(14, 12))\n",
        "\n",
        "    # Custom colormap\n",
        "    cmap = sns.light_palette(colors['primary'], as_cmap=True)\n",
        "\n",
        "    sns.heatmap(matrix, cmap=cmap, annot=True, fmt='d',\n",
        "                cbar_kws={'label': 'Co-occurrence Count'},\n",
        "                linewidths=0.5)\n",
        "\n",
        "    plt.title('Code Co-occurrence Heatmap', fontsize=16, fontweight='bold',\n",
        "              color=colors['primary'], pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{config['output_folder']}/code_cooccurrence.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_code_network_graph(df_coded, colors, min_cooccurrence=3):\n",
        "    \"\"\"\n",
        "    Create network graph of code relationships.\n",
        "    \"\"\"\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Build network\n",
        "    for idx, row in df_coded.iterrows():\n",
        "        if pd.notna(row['All_Codes']) and row['All_Codes']:\n",
        "            codes = [c.strip() for c in str(row['All_Codes']).split(',')]\n",
        "\n",
        "            # Add nodes with type\n",
        "            for code in codes:\n",
        "                if code not in G:\n",
        "                    node_type = 'inductive' if code.endswith('_IND') else 'deductive'\n",
        "                    G.add_node(code, type=node_type)\n",
        "\n",
        "            # Add edges\n",
        "            for i in range(len(codes)):\n",
        "                for j in range(i+1, len(codes)):\n",
        "                    if G.has_edge(codes[i], codes[j]):\n",
        "                        G[codes[i]][codes[j]]['weight'] += 1\n",
        "                    else:\n",
        "                        G.add_edge(codes[i], codes[j], weight=1)\n",
        "\n",
        "    # Filter edges\n",
        "    edges_to_remove = [(u, v) for u, v, d in G.edges(data=True)\n",
        "                       if d['weight'] < min_cooccurrence]\n",
        "    G.remove_edges_from(edges_to_remove)\n",
        "    G.remove_nodes_from(list(nx.isolates(G)))\n",
        "\n",
        "    if len(G.nodes()) > 0:\n",
        "        # Create visualization\n",
        "        plt.figure(figsize=(16, 12))\n",
        "        pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
        "\n",
        "        # Color nodes by type\n",
        "        node_colors = [colors['primary'] if G.nodes[node]['type'] == 'deductive' else colors['secondary']\n",
        "                       for node in G.nodes()]\n",
        "\n",
        "        # Size nodes by degree\n",
        "        node_sizes = [G.degree(node) * 100 for node in G.nodes()]\n",
        "\n",
        "        # Draw network\n",
        "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.7)\n",
        "\n",
        "        # Draw edges with width based on weight\n",
        "        edges = G.edges()\n",
        "        weights = [G[u][v]['weight'] for u, v in edges]\n",
        "        nx.draw_networkx_edges(G, pos, width=weights, alpha=0.3)\n",
        "\n",
        "        # Draw labels\n",
        "        nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "        plt.title(f'Code Co-occurrence Network (min {min_cooccurrence} co-occurrences)',\n",
        "                  fontsize=16, fontweight='bold', color=colors['primary'])\n",
        "        plt.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{config['output_folder']}/code_network.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "rvGwUVzOFHRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Functions\n",
        "\n",
        "Export analysis results to Excel and create formatted Word documents with themes."
      ],
      "metadata": {
        "id": "wC1yflgNFIqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_complete_analysis(df_coded, themes, inductive_results, code_patterns,\n",
        "                           integration_stats, coder):\n",
        "    \"\"\"\n",
        "    Export all analysis results to Excel and Word.\n",
        "    \"\"\"\n",
        "    print(\"\\nüíæ EXPORTING ANALYSIS RESULTS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    timestamp = config['timestamp']\n",
        "    excel_filename = f\"{config['output_folder']}/coding_analysis_{timestamp}.xlsx\"\n",
        "    word_filename = f\"{config['output_folder']}/themes_report_{timestamp}.docx\"\n",
        "\n",
        "    # Export to Excel\n",
        "    try:\n",
        "        with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
        "            # 1. Coded Data\n",
        "            df_coded.to_excel(writer, sheet_name='Coded_Data', index=False)\n",
        "\n",
        "            # 2. Coding Summary\n",
        "            summary_data = {\n",
        "                'Metric': ['Total Chunks', 'Deductive Only', 'Inductive Only',\n",
        "                          'Both Types', 'No Codes', 'Total Coded'],\n",
        "                'Count': [integration_stats['total_chunks'],\n",
        "                         integration_stats['deductive_only'],\n",
        "                         integration_stats['inductive_only'],\n",
        "                         integration_stats['both_types'],\n",
        "                         integration_stats['no_codes'],\n",
        "                         integration_stats['coded_chunks']]\n",
        "            }\n",
        "            summary_df = pd.DataFrame(summary_data)\n",
        "            summary_df.to_excel(writer, sheet_name='Coding_Summary', index=False)\n",
        "\n",
        "            # 3. Deductive Codebook\n",
        "            if coder and hasattr(coder, 'codebook_df'):\n",
        "                coder.codebook_df.to_excel(writer, sheet_name='Deductive_Codebook', index=False)\n",
        "\n",
        "            # 4. Inductive Codes\n",
        "            if inductive_results and 'discovered_codes' in inductive_results:\n",
        "                inductive_codes_data = []\n",
        "                for code, details in inductive_results['discovered_codes'].items():\n",
        "                    inductive_codes_data.append({\n",
        "                        'Code': code,\n",
        "                        'Definition': details.get('definition', ''),\n",
        "                        'Rationale': details.get('rationale', ''),\n",
        "                        'Application': details.get('application', ''),\n",
        "                        'Example': details.get('example', '')\n",
        "                    })\n",
        "                if inductive_codes_data:\n",
        "                    inductive_codes_df = pd.DataFrame(inductive_codes_data)\n",
        "                    inductive_codes_df.to_excel(writer, sheet_name='Inductive_Codes', index=False)\n",
        "\n",
        "            # 5. Code Frequencies\n",
        "            freq_data = []\n",
        "            for code, freq in code_patterns['all_codes_frequency'].items():\n",
        "                code_type = 'Inductive' if code.endswith('_IND') else 'Deductive'\n",
        "                freq_data.append({\n",
        "                    'Code': code,\n",
        "                    'Type': code_type,\n",
        "                    'Frequency': freq\n",
        "                })\n",
        "            freq_df = pd.DataFrame(freq_data)\n",
        "            freq_df.to_excel(writer, sheet_name='Code_Frequencies', index=False)\n",
        "\n",
        "            # 6. Code Combinations\n",
        "            if 'frequent_combinations' in code_patterns:\n",
        "                combo_data = []\n",
        "                for combo, freq in list(code_patterns['frequent_combinations'].items())[:20]:\n",
        "                    combo_data.append({\n",
        "                        'Combination': ' + '.join(combo),\n",
        "                        'Frequency': freq\n",
        "                    })\n",
        "                if combo_data:\n",
        "                    combo_df = pd.DataFrame(combo_data)\n",
        "                    combo_df.to_excel(writer, sheet_name='Code_Combinations', index=False)\n",
        "\n",
        "        print(f\"‚úÖ Excel file saved: {excel_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error exporting Excel: {e}\")\n",
        "\n",
        "    # Export themes to Word\n",
        "    try:\n",
        "        create_themes_document(themes, word_filename)\n",
        "        print(f\"‚úÖ Word document saved: {word_filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating Word document: {e}\")\n",
        "\n",
        "    print(\"\\nüìÅ Files created in folder:\")\n",
        "    print(f\"   {config['output_folder']}/\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ coding_analysis_[timestamp].xlsx\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ themes_report_[timestamp].docx\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ coding_distribution.png\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ code_frequencies.png\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ code_cooccurrence.png\")\n",
        "    if os.path.exists(f\"{config['output_folder']}/code_network.png\"):\n",
        "        print(\"   ‚îî‚îÄ‚îÄ code_network.png\")\n",
        "\n",
        "\n",
        "def create_themes_document(themes, filename):\n",
        "    \"\"\"\n",
        "    Create a formatted Word document with themes.\n",
        "    \"\"\"\n",
        "    doc = Document()\n",
        "\n",
        "    # Add title\n",
        "    title = doc.add_heading('Qualitative Analysis Themes Report', 0)\n",
        "    title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "    # Add metadata\n",
        "    doc.add_paragraph(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    doc.add_paragraph(f\"Analysis Type: {config['coding_approach'].title()}\")\n",
        "    doc.add_paragraph(f\"AI Model Used: {'Yes - ' + config['ai_model'] if config['use_ai'] else 'No'}\")\n",
        "\n",
        "    doc.add_page_break()\n",
        "\n",
        "    # Add themes\n",
        "    doc.add_heading('Discovered Themes', 1)\n",
        "\n",
        "    if themes and 'themes_analysis' in themes:\n",
        "        themes_text = themes['themes_analysis']\n",
        "\n",
        "        # Parse and format themes\n",
        "        lines = themes_text.split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line.startswith('THEME'):\n",
        "                # Main theme heading\n",
        "                doc.add_heading(line, 2)\n",
        "\n",
        "            elif line.startswith('Core Concept:'):\n",
        "                # Core concept paragraph\n",
        "                p = doc.add_paragraph()\n",
        "                p.add_run('Core Concept: ').bold = True\n",
        "                p.add_run(line.replace('Core Concept:', '').strip())\n",
        "\n",
        "            elif line.startswith('Sub-themes:'):\n",
        "                # Sub-themes heading\n",
        "                p = doc.add_paragraph()\n",
        "                p.add_run('Sub-themes:').bold = True\n",
        "\n",
        "            elif line.strip().startswith(('a)', 'b)', 'c)')):\n",
        "                # Sub-theme items\n",
        "                doc.add_paragraph(line, style='List Bullet')\n",
        "\n",
        "            elif line.startswith('Key Finding:'):\n",
        "                # Key finding\n",
        "                p = doc.add_paragraph()\n",
        "                p.add_run('Key Finding: ').bold = True\n",
        "                p.add_run(line.replace('Key Finding:', '').strip())\n",
        "\n",
        "            elif line.startswith('Evidence Strength:'):\n",
        "                # Evidence strength\n",
        "                p = doc.add_paragraph()\n",
        "                p.add_run('Evidence Strength: ').bold = True\n",
        "                p.add_run(line.replace('Evidence Strength:', '').strip())\n",
        "                doc.add_paragraph()  # Add spacing\n",
        "\n",
        "            elif line and not line.isspace():\n",
        "                # Other content\n",
        "                doc.add_paragraph(line)\n",
        "\n",
        "    else:\n",
        "        doc.add_paragraph(\"No themes were generated in this analysis.\")\n",
        "\n",
        "    # Save document\n",
        "    doc.save(filename)"
      ],
      "metadata": {
        "id": "yHtku88VFPI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Analysis Execution\n",
        "\n",
        "Run the complete analysis workflow based on the configured settings. This cell orchestrates all the analysis steps from coding through theme building and export."
      ],
      "metadata": {
        "id": "UbvlyO6JFOod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_complete_analysis():\n",
        "    \"\"\"\n",
        "    Run the complete analysis workflow based on configuration.\n",
        "    \"\"\"\n",
        "    print(\"\\nüöÄ STARTING QUALITATIVE CODING ANALYSIS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        # Check if files are loaded\n",
        "        if config['codebook_df'] is None or config['transcript_df'] is None:\n",
        "            print(\"‚ùå Please load files using the configuration interface first!\")\n",
        "            return None\n",
        "\n",
        "        # Initialize the deductive coder\n",
        "        coder = DeductiveCoder(config['codebook_df'])\n",
        "        coder.display_codebook_summary()\n",
        "\n",
        "        # Initialize results variables\n",
        "        df_coded = config['transcript_df'].copy()\n",
        "        inductive_results = None\n",
        "\n",
        "        # Determine coding approach\n",
        "        approach = config['coding_approach'].lower()\n",
        "\n",
        "        # Step 1: Deductive Coding (if not inductive only)\n",
        "        if 'inductive only' not in approach:\n",
        "            print(\"\\nüìù PHASE 1: DEDUCTIVE CODING\")\n",
        "            claude_coder = ClaudeAutoCoder(\n",
        "                api_key=config['api_key'],\n",
        "                codebook_df=config['codebook_df'],\n",
        "                coder=coder,\n",
        "                use_ai=config['use_ai']\n",
        "            )\n",
        "            df_coded = claude_coder.code_batch(df_coded)\n",
        "        else:\n",
        "            claude_coder = None\n",
        "\n",
        "        # Step 2: Inductive Coding (if not deductive only)\n",
        "        if 'deductive only' not in approach:\n",
        "            print(\"\\nüìù PHASE 2: INDUCTIVE CODING\")\n",
        "            # Need a claude_coder instance even for inductive only\n",
        "            if claude_coder is None:\n",
        "                claude_coder = ClaudeAutoCoder(\n",
        "                    api_key=config['api_key'],\n",
        "                    codebook_df=config['codebook_df'],\n",
        "                    coder=coder,\n",
        "                    use_ai=config['use_ai']\n",
        "                )\n",
        "\n",
        "            inductive_coder = ClaudeInductiveCoder(claude_coder, df_coded)\n",
        "            inductive_results = inductive_coder.generate_inductive_codes(sample_size=60)\n",
        "\n",
        "            # Display discovered codes\n",
        "            if 'discovered_codes' in inductive_results:\n",
        "                print(\"\\nüìã Discovered Inductive Codes:\")\n",
        "                for code, details in inductive_results['discovered_codes'].items():\n",
        "                    print(f\"\\n‚Ä¢ {code}\")\n",
        "                    print(f\"  Definition: {details['definition']}\")\n",
        "                    print(f\"  When to apply: {details['application']}\")\n",
        "\n",
        "            # Apply inductive codes\n",
        "            df_coded = inductive_coder.apply_inductive_codes()\n",
        "\n",
        "        # Step 3: Integrate coding approaches\n",
        "        print(\"\\nüìù PHASE 3: INTEGRATION & ANALYSIS\")\n",
        "        df_coded, integration_stats = integrate_coding_approaches(df_coded)\n",
        "\n",
        "        # Step 4: Analyze patterns\n",
        "        code_patterns = analyze_code_patterns(df_coded)\n",
        "\n",
        "        # Step 5: Build themes\n",
        "        print(\"\\nüìù PHASE 4: THEME BUILDING\")\n",
        "        theme_builder = IntegratedThemeBuilder(\n",
        "            claude_coder=claude_coder,\n",
        "            coded_df=df_coded,\n",
        "            code_patterns=code_patterns,\n",
        "            inductive_results=inductive_results\n",
        "        )\n",
        "        themes = theme_builder.build_themes()\n",
        "\n",
        "        # Display themes\n",
        "        if 'themes_analysis' in themes:\n",
        "            print(\"\\nüìã GENERATED THEMES:\")\n",
        "            print(\"=\" * 60)\n",
        "            print(themes['themes_analysis'])\n",
        "\n",
        "        # Step 6: Create visualizations\n",
        "        print(\"\\nüìù PHASE 5: VISUALIZATIONS\")\n",
        "        create_comprehensive_visualizations(df_coded, code_patterns, themes, coder)\n",
        "\n",
        "        # Step 7: Export results\n",
        "        print(\"\\nüìù PHASE 6: EXPORT\")\n",
        "        export_complete_analysis(\n",
        "            df_coded=df_coded,\n",
        "            themes=themes,\n",
        "            inductive_results=inductive_results,\n",
        "            code_patterns=code_patterns,\n",
        "            integration_stats=integration_stats,\n",
        "            coder=coder\n",
        "        )\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"üìä QUALITATIVE ANALYSIS COMPLETE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"\\n‚úÖ Analysis Type: {config['coding_approach']}\")\n",
        "        print(f\"‚úÖ Total chunks analyzed: {integration_stats['total_chunks']}\")\n",
        "        print(f\"‚úÖ Successfully coded: {integration_stats['coded_chunks']}\")\n",
        "        print(f\"‚úÖ Unique codes used: {code_patterns['unique_codes']}\")\n",
        "        print(f\"\\nüìÅ All results saved to: {config['output_folder']}/\")\n",
        "\n",
        "        return {\n",
        "            'df_coded': df_coded,\n",
        "            'themes': themes,\n",
        "            'code_patterns': code_patterns,\n",
        "            'integration_stats': integration_stats,\n",
        "            'inductive_results': inductive_results\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error in analysis workflow: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run the analysis when ready\n",
        "print(\"\\n‚úÖ Analysis function ready!\")\n",
        "print(\"üìå To run the analysis:\")\n",
        "print(\"1. Upload your files using the configuration interface above\")\n",
        "print(\"2. Configure your analysis settings\")\n",
        "print(\"3. Click 'Start Analysis' button\")\n",
        "print(\"4. Then run: results = run_complete_analysis()\")"
      ],
      "metadata": {
        "id": "yPZSp65dFUv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execute\n"
      ],
      "metadata": {
        "id": "29-Ykx0JE8gH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the complete analysis\n",
        "results = run_complete_analysis()"
      ],
      "metadata": {
        "id": "mJnCKVMkFb9f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}